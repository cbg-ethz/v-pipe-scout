"""Implements the Wiseloculus API Queries."""

import logging
import aiohttp
import asyncio
import re
from typing import Optional, List, Tuple, Any
from datetime import datetime, timedelta

import pandas as pd

from .lapis import Lapis
from .exceptions import APIError
from interface import MutationType

# Constants for fallback date range
# When API fails, use the last 3 months instead of an entire year to avoid huge API calls
def get_fallback_date_range() -> Tuple[datetime, datetime]:
    """
    Returns fallback date range of the last 3 months from today.
    This avoids excessively large API calls when the API date range query fails.
    """
    end_date = datetime.now()
    start_date = end_date - timedelta(days=90)  # Approximately 3 months
    return start_date, end_date

FALLBACK_START_DATE, FALLBACK_END_DATE = get_fallback_date_range()

class WiseLoculusLapis(Lapis):
    """Wise-Loculus Instance API"""

    def _mutations_to_and_query(self, mutations: List[str]) -> str:
        """
        Convert a list of mutations to an AND query string for advancedQuery.
        
        Args:
            mutations: List of mutations (e.g., ["23149T", "23224T", "23311T"])
            
        Returns:
            str: AND query string (e.g., "23149T & 23224T & 23311T")
            
        Examples:
            >>> _mutations_to_and_query(["23149T"])
            "23149T"
            >>> _mutations_to_and_query(["23149T", "23224T", "23311T"])
            "23149T & 23224T & 23311T"
            >>> _mutations_to_and_query([])
            ""
        """
        if not mutations:
            return ""
        if len(mutations) == 1:
            return mutations[0]
        return " & ".join(mutations)

    def _extract_position(self, mutation: str) -> Optional[str]:
        """
        Extract numeric genomic position from a mutation string.
        Examples:
        - "A13T" -> "13"
        - "301-" -> "301"
        - "303" -> "303"
        Returns None if no position is found.
        """
        m = re.search(r"(\d+)", str(mutation))
        return m.group(1) if m else None

    def _intersection_coverage_query(self, mutations: List[str]) -> str:
        """
        Build an advancedQuery ensuring reads have non-N calls at ALL positions.
        For positions {13, 301, 303} â†’ "!13N & !301N & !303N".
        """
        positions: List[str] = []
        for mut in mutations:
            pos = self._extract_position(mut)
            if pos:
                positions.append(pos)
        if not positions:
            return ""
        # De-duplicate and sort for stability
        unique_positions = sorted(set(positions), key=int)  
        return " & ".join([f"!{p}N" for p in unique_positions])

    async def sample_mutations(
            self, 
            type: MutationType,
            date_range: Tuple[datetime, datetime], 
            locationName: Optional[str] = None,
            min_proportion: float = 0.01,
            nucleotide_mutations: Optional[List[str]] = None,
            amino_acid_mutations: Optional[List[str]] = None,
        ) -> pd.DataFrame:
        """
        Fetches nucleotide mutations for a given date range and optional location.
        Fetches mutations (nucleotide or amino acid) for a given date range and optional location.
        Filters for sequences/reads with particular nucleotide or amino acid mutations, depending on the specified mutation type and provided filters.
        
        Returns a DataFrame with 
        Columns: ['mutation', 'count', 'coverage', 'proportion', 'sequenceName', 'mutationFrom', 'mutationTo', 'position']
        """

        payload = {
            "samplingDateFrom": date_range[0].strftime('%Y-%m-%d'),
            "samplingDateTo": date_range[1].strftime('%Y-%m-%d'),
            "locationName": locationName,
            "minProportion": min_proportion, 
            "orderBy": "proportion",
            "limit": 10000,  # Adjust limit as needed
            "dataFormat": "JSON",
            "downloadAsFile": "false"
        }

        # Add mutation filters if provided
        if nucleotide_mutations:
            payload["nucleotideMutations"] = nucleotide_mutations
        if amino_acid_mutations:
            payload["aminoAcidMutations"] = amino_acid_mutations

        if type == MutationType.AMINO_ACID:
            endpoint = f'{self.server_ip}/sample/aminoAcidMutations'
        elif type == MutationType.NUCLEOTIDE:
            endpoint = f'{self.server_ip}/sample/nucleotideMutations'
        else:
            logging.error(f"Unknown mutation type: {type}")
            return pd.DataFrame()

        try:
            timeout = aiohttp.ClientTimeout(total=30) 
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.get(
                    endpoint,
                    params=payload,
                    headers={'accept': 'application/json'}
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        df = pd.DataFrame(data['data'])
                        return df
                    else:
                        logging.error(f"Failed to fetch nucleotide mutations: {response.status}")
                        return pd.DataFrame()
        except Exception as e:
            logging.error(f"Error fetching mutations: {e}")
            return pd.DataFrame()
    

    async def get_date_range(self) -> Tuple[Optional[datetime], Optional[datetime]]:
        """
        Fetches all available sampling dates and returns the earliest and latest dates.
        
        Returns:
            Tuple[Optional[datetime], Optional[datetime]]: (earliest_date, latest_date) or (None, None) if no data
        """
        try:
            timeout = aiohttp.ClientTimeout(total=30)  # 30 second timeout for this query
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.get(
                    f'{self.server_ip}/sample/aggregated',
                    params={'fields': 'samplingDate'},
                    headers={'accept': 'application/json'}
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        sample_data = data.get('data', [])
                        
                        if not sample_data:
                            logging.warning("No sampling date data available")
                            return None, None
                        
                        # Extract all dates and convert to datetime objects
                        dates = []
                        for entry in sample_data:
                            if 'samplingDate' in entry:
                                try:
                                    date_obj = datetime.strptime(entry['samplingDate'], '%Y-%m-%d')
                                    dates.append(date_obj)
                                except ValueError as e:
                                    logging.warning(f"Invalid date format: {entry['samplingDate']}: {e}")
                        
                        if not dates:
                            logging.warning("No valid sampling dates found")
                            return None, None
                        
                        earliest_date = min(dates)
                        latest_date = max(dates)
                        
                        logging.info(f"Date range: {earliest_date.strftime('%Y-%m-%d')} to {latest_date.strftime('%Y-%m-%d')}")
                        return earliest_date, latest_date
                        
                    else:
                        logging.error(f"Failed to fetch sampling dates: {response.status}")
                        logging.error(await response.text())
                        return None, None
                        
        except Exception as e:
            logging.error(f"Error fetching date range: {e}")
            return None, None

    def get_cached_date_range(self, cache_key: str = "default") -> Tuple[datetime, datetime]:
        """
        Get the date range with caching to avoid repeated API calls.
        Returns pandas Timestamps for compatibility with Streamlit date inputs.
        
        Args:
            cache_key: Unique key for this cache (allows multiple cached ranges)
            
        Returns:
            Tuple[datetime, datetime]: Start and end dates as pandas Timestamps
        """
        import streamlit as st
        import asyncio
        import pandas as pd
        
        # Create a unique session state key
        session_key = f"wiseloculus_date_range_{cache_key}"
        
        # Check if we already have cached date range
        if session_key in st.session_state:
            cached_range = st.session_state[session_key]
            logging.debug(f"Using cached date range for {cache_key}: {cached_range}")
            return cached_range
        
        # Fetch new date range
        try:
            earliest, latest = asyncio.run(self.get_date_range())
            
            if earliest and latest:
                # Convert to pandas Timestamps for Streamlit compatibility
                date_range = (pd.to_datetime(earliest), pd.to_datetime(latest))
                logging.info(f"Fetched date range for {cache_key}: {date_range[0].strftime('%Y-%m-%d')} to {date_range[1].strftime('%Y-%m-%d')}")
            else:
                # Fallback to default dates
                date_range = (pd.to_datetime(FALLBACK_START_DATE), pd.to_datetime(FALLBACK_END_DATE))
                logging.warning(f"API date range not available for {cache_key}, using defaults: {date_range}")
                
        except Exception as e:
            # Fallback to default dates
            date_range = (pd.to_datetime(FALLBACK_START_DATE), pd.to_datetime(FALLBACK_END_DATE))
            logging.warning(f"Error fetching date range for {cache_key}: {e}, using defaults")
        
        # Cache the result
        st.session_state[session_key] = date_range
        return date_range

    def get_cached_date_range_with_bounds(self, cache_key: str = "default") -> Tuple[datetime, datetime, datetime, datetime]:
        """
        Get the date range with bounds for enforcing min/max in date inputs.
        
        Args:
            cache_key: Unique key for this cache
            
        Returns:
            Tuple[datetime, datetime, datetime, datetime]: (start_date, end_date, min_date, max_date)
        """
        start_date, end_date = self.get_cached_date_range(cache_key)
        
        # Use the same dates as bounds to enforce API limits
        # Add a small buffer for edge cases (1 day on each side)
        import pandas as pd
        buffer = pd.Timedelta(days=1)
        min_date = start_date - buffer
        max_date = end_date + buffer
        
        return start_date, end_date, min_date, max_date
    

    async def _component_mutations_over_time(
            self,
            endpoint: str,
            mutation_type_name: str,
            mutations: List[str], 
            date_ranges: List[Tuple[datetime, datetime]],
            locationName: str
        ) -> dict[str, Any]:
        """
        Helper method for fetching mutations over time data from component endpoints.
        
        Args:
            endpoint: The API endpoint name (e.g., "aminoAcidMutationsOverTime")
            mutation_type_name: Display name for logging (e.g., "amino acid")
            mutations: List of mutations
            date_ranges: List of date range tuples
            locationName: Location name to filter by
            
        Returns:
            Dict containing the API response with mutations, dateRanges, and data matrix
        """
        payload = {
            "filters": {
                "locationName": locationName
            },
            "includeMutations": mutations,
            "dateRanges": [
                {
                    "dateFrom": date_range[0].strftime('%Y-%m-%d'),
                    "dateTo": date_range[1].strftime('%Y-%m-%d')
                }
                for date_range in date_ranges
            ],
            "dateField": "samplingDate"
        }

        logging.debug(f"Fetching {mutation_type_name} mutations over time with payload: {payload}")
        
        try:
            timeout = aiohttp.ClientTimeout(total=30)  # 30 second timeout (consistent with other API calls)
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.post(
                    f'{self.server_ip}/component/{endpoint}',
                    headers={
                        'accept': 'application/json',
                        'Content-Type': 'application/json'
                    },
                    json=payload
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        return data
                    elif response.status == 500:
                        # Log the failed query for debugging
                        logging.error(f"Internal Server Error (500) for {mutation_type_name} mutations over time")
                        logging.error(f"Failed POST query: {self.server_ip}/component/{endpoint}")
                        logging.error(f"Payload: {payload}")
                        error_text = await response.text()
                        logging.error(f"Server response: {error_text}")
                        
                        # Raise custom APIError for better frontend handling
                        raise APIError(
                            f"Internal Server Error: The backend API server is experiencing issues. This is not an application error.",
                            status_code=500,
                            details=error_text,
                            payload=payload
                        )
                    else:
                        logging.error(f"Failed to fetch {mutation_type_name} mutations over time.")
                        logging.error(f"Status code: {response.status}")
                        error_text = await response.text()
                        logging.error(error_text)
                        raise APIError(
                            f"API request failed with status {response.status}",
                            status_code=response.status,
                            details=error_text,
                            payload=payload
                        )
        except APIError:
            # Re-raise our custom APIError
            raise
        except Exception as e:
            logging.error(f"Connection error fetching {mutation_type_name} mutations over time: {e}")
            raise APIError(
                f"Connection error: {str(e)}",
                details=str(e),
                payload=payload
            )

    async def component_aminoAcidMutationsOverTime(
            self, 
            mutations: List[str], 
            date_ranges: List[Tuple[datetime, datetime]],
            locationName: str
        ) -> dict[str, Any]:
        """
        Fetches amino acid mutations over time for a given location and specific date ranges.
        Returns counts and coverage for each mutation and date range.
        
        Args:
            mutations: List of amino acid mutations in format ["S:N501Y", "N:N8N"]
            date_ranges: List of date range tuples [(start_date, end_date), ...]
            locationName: Location name to filter by
            
        Returns:
            Dict containing the API response with mutations, dateRanges, and data matrix
        """
        return await self._component_mutations_over_time(
            endpoint="aminoAcidMutationsOverTime",
            mutation_type_name="amino acid",
            mutations=mutations,
            date_ranges=date_ranges,
            locationName=locationName
        )

    async def component_nucleotideMutationsOverTime(
            self, 
            mutations: List[str], 
            date_ranges: List[Tuple[datetime, datetime]],
            locationName: str
        ) -> dict[str, Any]:
        """
        Fetches nucleotide mutations over time for a given location and specific date ranges.
        Returns counts and coverage for each mutation and date range.
        
        Args:
            mutations: List of nucleotide mutations in format ["A5341C", "C34G"]
            date_ranges: List of date range tuples [(start_date, end_date), ...]
            locationName: Location name to filter by
            
        Returns:
            Dict containing the API response with mutations, dateRanges, and data matrix
        """
        return await self._component_mutations_over_time(
            endpoint="nucleotideMutationsOverTime",
            mutation_type_name="nucleotide",
            mutations=mutations,
            date_ranges=date_ranges,
            locationName=locationName
        )

    def _generate_date_ranges(
            self, 
            date_range: Tuple[datetime, datetime], 
            interval: str = "daily"
        ) -> List[Tuple[datetime, datetime]]:
        """
        Generate date ranges based on the specified interval.
        
        Args:
            date_range: Tuple of (start_date, end_date)
            interval: "daily", "weekly", or "monthly"
            
        Returns:
            List of date range tuples
        """
        start_date, end_date = date_range
        date_ranges = []
        
        if interval == "daily":
            current_date = start_date
            while current_date <= end_date:
                date_ranges.append((current_date, current_date))
                current_date += pd.Timedelta(days=1)
                
        elif interval == "weekly":
            current_date = start_date
            while current_date <= end_date:
                week_end = min(current_date + pd.Timedelta(days=6), end_date)
                date_ranges.append((current_date, week_end))
                current_date = week_end + pd.Timedelta(days=1)
                
        elif interval == "monthly":
            current_date = start_date
            while current_date <= end_date:
                # Get the last day of the current month
                if current_date.month == 12:
                    next_month = current_date.replace(year=current_date.year + 1, month=1, day=1)
                else:
                    next_month = current_date.replace(month=current_date.month + 1, day=1)
                month_end = min(next_month - pd.Timedelta(days=1), end_date)
                date_ranges.append((current_date, month_end))
                current_date = next_month
                
        else:
            raise ValueError(f"Unsupported interval: {interval}. Use 'daily', 'weekly', or 'monthly'")
            
        return date_ranges

    async def mutations_over_time(
            self, 
            mutations: List[str], 
            mutation_type: MutationType, 
            date_range: Tuple[datetime, datetime], 
            locationName: str,
            interval: str = "daily"
        ) -> pd.DataFrame:
        """
        Fetches mutation counts, coverage, and frequency using component endpoints for specified time intervals.

        Args:
            mutations (List[str]): List of mutations to fetch data for.
            mutation_type (MutationType): Type of mutations (NUCLEOTIDE or AMINO_ACID).
            date_range (Tuple[datetime, datetime]): Tuple containing start and end dates for the data range.
            locationName (str): Location name to filter by.
            interval (str): Time interval - "daily" (default), "weekly", or "monthly".

        Returns:
            pd.DataFrame: A MultiIndex DataFrame with mutation and samplingDate as the index, 
                         and count, coverage, and frequency as columns.
        """
        try:
            # Generate date ranges based on the specified interval
            date_ranges = self._generate_date_ranges(date_range, interval)
            
            # Choose the appropriate component endpoint based on mutation type
            if mutation_type == MutationType.AMINO_ACID:
                api_data = await self.component_aminoAcidMutationsOverTime(mutations, date_ranges, locationName)
            elif mutation_type == MutationType.NUCLEOTIDE:
                api_data = await self.component_nucleotideMutationsOverTime(mutations, date_ranges, locationName)
            else:
                raise ValueError(f"Unsupported mutation type: {mutation_type}")

            # Parse the API response (no need to check for "error" key anymore as we raise APIError)
            records = []
            api_data_content = api_data.get("data", {})
            api_mutations = api_data_content.get("mutations", [])
            api_date_ranges = api_data_content.get("dateRanges", [])
            data_matrix = api_data_content.get("data", [])

            # Debug logging
            logging.debug(f"mutations_over_time: Received {len(api_mutations)} mutations, {len(api_date_ranges)} date ranges")

            # Process the data matrix
            for i, mutation in enumerate(api_mutations):
                for j, date_range_info in enumerate(api_date_ranges):
                    if i < len(data_matrix) and j < len(data_matrix[i]):
                        mutation_data = data_matrix[i][j]
                        
                        # Extract count and coverage from the API response
                        count = mutation_data.get("count", 0)
                        coverage = mutation_data.get("coverage", 0)
                        
                        # Calculate frequency from count and coverage
                        frequency = count / coverage if coverage > 0 else pd.NA
                        
                        # For interval-based data, use the start date as the samplingDate
                        # or use the midpoint for better representation
                        start_date = pd.to_datetime(date_range_info["dateFrom"])
                        end_date = pd.to_datetime(date_range_info["dateTo"])
                        
                        if interval == "daily":
                            samplingDate = start_date.strftime('%Y-%m-%d')
                        else:
                            # Use midpoint for weekly/monthly intervals
                            midpoint = start_date + (end_date - start_date) / 2
                            samplingDate = midpoint.strftime('%Y-%m-%d')
                        
                        # Add all records, even those with coverage = 0
                        # This allows us to see when data is missing vs when mutations don't exist
                        records.append({
                            "mutation": mutation,
                            "samplingDate": samplingDate,
                            "count": count,
                            "coverage": coverage,
                            "frequency": frequency
                        })

            logging.debug(f"Created {len(records)} records from component API data")

            # Create DataFrame from records
            df = pd.DataFrame(records)

            # Check for and handle duplicates before setting MultiIndex
            if not df.empty and "mutation" in df.columns and "samplingDate" in df.columns:
                # Check for duplicates
                duplicates = df.duplicated(subset=['mutation', 'samplingDate'], keep=False)
                if duplicates.any():
                    logging.warning(f"Found {duplicates.sum()} duplicate mutation-date combinations, removing duplicates")
                    # Keep first occurrence of each duplicate, preferring non-zero values
                    df = df.drop_duplicates(subset=['mutation', 'samplingDate'], keep='first')
                    logging.debug(f"After deduplication: {len(df)} records remain")
                
                df.set_index(["mutation", "samplingDate"], inplace=True)
            else:
                # Create empty DataFrame with proper MultiIndex structure
                df = pd.DataFrame(columns=["count", "coverage", "frequency"])
                df.index = pd.MultiIndex.from_tuples([], names=["mutation", "samplingDate"])
            return df

        except APIError as api_error:
            # Log the API error and return empty DataFrame
            logging.error(f"APIError encountered in mutations_over_time: {api_error}")
            df = pd.DataFrame(columns=["count", "coverage", "frequency"])
            df.index = pd.MultiIndex.from_tuples([], names=["mutation", "samplingDate"])
            return df
        except Exception as e:
            logging.error(f"Error in mutations_over_time: {e}")
            # Return empty DataFrame with proper MultiIndex structure
            df = pd.DataFrame(columns=["count", "coverage", "frequency"])
            df.index = pd.MultiIndex.from_tuples([], names=["mutation", "samplingDate"])
            return df

    async def coocurrences_over_time(
            self,
            mutations: List[str],
            date_range: Tuple[datetime, datetime],
            locationName: str,
            interval: str = "daily"
        ) -> pd.DataFrame:
        """
        Fetch proportion data for a SET of mutations (AND filter) over time.
        
        Unlike mutations_over_time which tracks individual mutations separately,
        this tracks all mutations together as a filter condition.
        
        Strategy:
        1. Use advanced queries to get coverage: reads covering ALL positions simultaneously ("!posN" AND ...)
        2. Query for count of reads matching ALL mutations (AND)
        3. Calculate frequency = count / coverage
        
        Args:
            mutations: List of nucleotide mutations to filter by (AND condition)
            date_range: Tuple of (start_date, end_date)
            locationName: Location name to filter by
            interval: "daily", "weekly", or "monthly"
            
        Returns:
            DataFrame with columns: samplingDate, count, coverage, frequency 
        """
        try:
            # Step 1: Query for reads matching ALL mutations (AND filter) and intersection coverage simultaneously
            date_ranges = self._generate_date_ranges(date_range, interval)
            
            filtered_lookup = {}  # date -> count of reads with ALL mutations
            coverage_lookup = {}  # date -> reads covering all positions
            
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=60)) as session:
                tasks = []
                
                for date_start, date_end in date_ranges:
                    # Task 1: Filtered count (reads with ALL mutations)
                    filtered_payload = {
                        "locationName": locationName,
                        "samplingDateFrom": date_start.strftime('%Y-%m-%d'),
                        "samplingDateTo": date_end.strftime('%Y-%m-%d'),
                        "advancedQuery": self._mutations_to_and_query(mutations),  # ALL mutations must match (AND)
                        "fields": ["samplingDate"]
                    }
                    
                    filtered_task = session.post(
                        f'{self.server_ip}/sample/aggregated',
                        headers={'accept': 'application/json', 'Content-Type': 'application/json'},
                        json=filtered_payload
                    )
                    
                    # Task 2: Intersection coverage (reads covering all positions)
                    # Require non-N calls at all positions: !posN & !posN ...
                    intersection_payload = {
                        "locationName": locationName,
                        "samplingDateFrom": date_start.strftime('%Y-%m-%d'),
                        "samplingDateTo": date_end.strftime('%Y-%m-%d'),
                        "advancedQuery": self._intersection_coverage_query(mutations),
                        "fields": ["samplingDate"]
                    }
                    
                    intersection_task = session.post(
                        f'{self.server_ip}/sample/aggregated',
                        headers={'accept': 'application/json', 'Content-Type': 'application/json'},
                        json=intersection_payload
                    )
                    
                    tasks.append((filtered_task, intersection_task))
                
                # Execute all queries in parallel
                all_results = await asyncio.gather(*[task for pair in tasks for task in pair], return_exceptions=True)
                
                # Process results in pairs (filtered, intersection)
                for idx in range(0, len(all_results), 2):
                    filtered_resp = all_results[idx]
                    intersection_resp = all_results[idx + 1]
                    
                    # Process filtered response
                    if not isinstance(filtered_resp, Exception) and filtered_resp.status == 200:
                        filtered_data = await filtered_resp.json()
                        filtered_items = filtered_data.get('data', [])
                        
                        for item in filtered_items:
                            date_str = item.get('samplingDate')
                            count = item.get('count', 0)
                            if date_str:
                                filtered_lookup[date_str] = count
                    else:
                        if isinstance(filtered_resp, Exception):
                            logging.error(f"Error fetching filtered data: {filtered_resp}")
                        else:
                            logging.error(f"API error for filtered data: status={filtered_resp.status}")
                    
                    # Process coverage (intersection) response
                    if not isinstance(intersection_resp, Exception) and intersection_resp.status == 200:
                        intersection_data = await intersection_resp.json()
                        intersection_items = intersection_data.get('data', [])
                        
                        for item in intersection_items:
                            date_str = item.get('samplingDate')
                            count = item.get('count', 0)
                            if date_str:
                                coverage_lookup[date_str] = count
                    else:
                        if isinstance(intersection_resp, Exception):
                            logging.error(f"Error fetching intersection coverage: {intersection_resp}")
                        else:
                            logging.error(f"API error for intersection coverage: status={intersection_resp.status}")
            
            # Step 2: Combine coverage and filtered count to calculate frequency
            records = []
            for date_str, coverage in coverage_lookup.items():
                filtered_count = filtered_lookup.get(date_str, 0)
                if coverage > 0:
                    frequency = filtered_count / coverage
                    records.append({
                        'samplingDate': pd.to_datetime(date_str),
                        'count': filtered_count,
                        'coverage': coverage,
                        'frequency': frequency
                    })
            
            # Create DataFrame
            if records:
                df = pd.DataFrame(records)
                df = df.sort_values('samplingDate')
                return df
            else:
                return pd.DataFrame(columns=['samplingDate', 'count', 'coverage', 'frequency'])
                
        except Exception as e:
            logging.error(f"Error in coocurrences_over_time: {e}")
            import traceback
            logging.error(traceback.format_exc())
            return pd.DataFrame(columns=['samplingDate', 'count', 'coverage', 'frequency'])